{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a7a3aeb6-fdee-4ee7-8d85-a398e0029f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5860728-9ea6-4157-9fba-4aaebda3b40f",
   "metadata": {},
   "source": [
    "| **Line of Code**                               | **What It Does**                                                                                                  | **Simple Analogy**                                                                                |\n",
    "| ---------------------------------------------- | ----------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| `import torch`                                 | Brings in **PyTorch**, a library for tensors and deep learning.                                                   | Like opening a **big toolbox** for machine learning.                                              |\n",
    "| `import torch.nn as nn`                        | Imports the **neural network building blocks** (layers, activations, etc.) and gives them the nickname `nn`.      | Like taking out a box of **Lego blocks** to build neural networks.                                |\n",
    "| `import torch.optim as optim`                  | Imports **optimizers**, which help the model learn by adjusting weights step by step. Nickname = `optim`.         | Like having a **guide** who tells you which way to walk when climbing a hill.                     |\n",
    "| `from torchvision import datasets, transforms` | `datasets`: ready-to-use image collections (like MNIST). `transforms`: tools to resize, convert, or clean images. | `datasets` = a **library of books**, `transforms` = the **photocopier/scanner** to prepare pages. |\n",
    "| `from torch.utils.data import DataLoader`      | Splits datasets into **mini-batches**, shuffles them, and feeds them to the network.                              | Like slicing a **big pizza** into smaller pieces so it’s easier to eat.                           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcd3714f-1823-4250-b28d-ca4fc38edfd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the datasets\n",
    "train_dataset = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = True,\n",
    "    transform = transforms.ToTensor(),\n",
    "    download = True\n",
    ")\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    root = \"data\",\n",
    "    train = False,\n",
    "    transform = transforms.ToTensor(),\n",
    "    download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e972e7-2e50-4b4a-a60f-7e70ac4c4947",
   "metadata": {},
   "source": [
    "| **Line of Code**                      | **What It Does**                                                                                | **Simple Analogy**                                                                              |\n",
    "| ------------------------------------- | ----------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| `# Load the datasets`                 | A **comment** (not code). It’s just a note to remind us that the next lines will load datasets. | Like putting a **sticky note** on a page saying “Here’s where we load the data.”                |\n",
    "| `train_dataset = datasets.MNIST(...)` | Loads the **MNIST training set** (60,000 images of handwritten digits 0–9).                     | Like getting a **practice workbook** full of math problems to train on.                         |\n",
    "| `root = \"data\"`                       | Tells PyTorch to **store/download** the dataset inside a folder called `\"data\"`.                | Like choosing a **folder on your computer** where you’ll keep your homework.                    |\n",
    "| `train = True`                        | Says we want the **training portion** of MNIST (used for learning).                             | Like using the **practice problems** section of a textbook.                                     |\n",
    "| `transform = transforms.ToTensor()`   | Converts images into **tensors** (so PyTorch can understand them as numbers).                   | Like scanning a **paper photo** and turning it into a **digital image** your computer can read. |\n",
    "| `download = True`                     | If MNIST isn’t already in `\"data\"`, it will **download it from the internet**.                  | Like saying “If I don’t have this workbook, go buy it online.”                                  |\n",
    "| `test_dataset = datasets.MNIST(...)`  | Loads the **MNIST testing set** (10,000 images of handwritten digits).                          | Like having a **final exam paper** to check if you really learned from practice.                |\n",
    "| `train = False`                       | Means this time we want the **test portion** of MNIST (used for evaluation).                    | Like opening the **exam questions section** of a book, not practice problems.                   |\n",
    "| *(Other arguments same as above)*     | Root folder, transform, and download work the same way as before.                               | Same as above.                                                                                  |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "723fa415-b18b-4a05-abcf-e543b805acaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wrap the datasets in dataloader for batching\n",
    "train_loader = DataLoader(dataset = train_dataset, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(dataset = test_dataset, batch_size = 64, shuffle = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302bf54c-3b59-465f-a7a0-6bb636b1217a",
   "metadata": {},
   "source": [
    "| **Line of Code**                                 | **What It Does**                                                                                                | **Simple Analogy**                                                                                       |\n",
    "| ------------------------------------------------ | --------------------------------------------------------------------------------------------------------------- | -------------------------------------------------------------------------------------------------------- |\n",
    "| `# Wrap the datasets in dataloader for batching` | A **comment** to remind us the next lines are about batching data.                                              | Like writing a note: “Now we’ll cut the pizza into slices.”                                              |\n",
    "| `train_loader = DataLoader(...)`                 | Creates a **data loader** for the training set. It will feed the model small batches of images during training. | Like a **waiter** bringing food to the table plate by plate instead of dumping the whole buffet at once. |\n",
    "| `dataset = train_dataset`                        | Tells it to use the **training dataset** we created earlier.                                                    | Like saying “Serve food from the practice workbook.”                                                     |\n",
    "| `batch_size = 64`                                | Each batch will contain **64 images** at a time.                                                                | Like giving the student **64 practice problems** in one go instead of the whole book.                    |\n",
    "| `shuffle = True`                                 | Mixes the data randomly every time before giving it to the model.                                               | Like shuffling a deck of cards so the student doesn’t memorize the **order** of questions.               |\n",
    "| `test_loader = DataLoader(...)`                  | Creates a **data loader** for the test set.                                                                     | Like preparing the **exam questions** to be served in chunks.                                            |\n",
    "| `dataset = test_dataset`                         | Uses the **testing dataset**.                                                                                   | Like saying “Serve food from the exam paper.”                                                            |\n",
    "| `batch_size = 64`                                | Again, 64 test images at a time.                                                                                | Like checking the student’s answers on **64 exam questions** at once.                                    |\n",
    "| `shuffle = False`                                | Doesn’t shuffle test data (keeps it in the same order).                                                         | Like saying “The exam paper should stay in its original order, no shuffling.”                            |\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9659b657-a922-4c8b-9940-555f7beadecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simple neural network\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 128)  #nn.Linear(input_size, output_size)\n",
    "        self.fc2 = nn.Linear(128,10)\n",
    "    def forward(self,x):\n",
    "        x = x.view(-1, 28*28)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caa6c00f-6658-4e40-a04d-2e7f9996af5e",
   "metadata": {},
   "source": [
    "| **Line of Code**                   | **What It Does**                                                                                                              | **Simple Analogy**                                                                                |\n",
    "| ---------------------------------- | ----------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------- |\n",
    "| `# Define a simple neural network` | A **comment** to explain the next code defines the model.                                                                     | Like writing a label on a box: “This is where we design the brain.”                               |\n",
    "| `class SimpleNN(nn.Module):`       | Creates a new class called `SimpleNN` which is a type of **neural network model** (`nn.Module`).                              | Like saying “I’m building my own robot by extending the blueprint from a robot factory.”          |\n",
    "| `def __init__(self):`              | The **constructor function**. Runs once when you create the model.                                                            | Like setting up the robot’s body parts when it’s first built.                                     |\n",
    "| `super().__init__()`               | Calls the parent class (`nn.Module`) to set things up properly.                                                               | Like telling the factory: “Finish the standard setup before I add custom parts.”                  |\n",
    "| `self.fc1 = nn.Linear(28*28, 128)` | First layer: takes an input of size **28×28 pixels = 784 numbers** and maps them to **128 hidden units**.                     | Like converting a **large raw photo** into a **smaller summary** the robot can understand.        |\n",
    "| `self.fc2 = nn.Linear(128, 10)`    | Second layer: takes the 128 hidden values and maps them to **10 outputs** (digits 0–9).                                       | Like the robot deciding which of the **10 possible answers** (0–9) is most likely.                |\n",
    "| `def forward(self, x):`            | Defines **how data flows** through the network.                                                                               | Like saying “When you give my robot some input, here’s the step-by-step process it follows.”      |\n",
    "| `x = x.view(-1, 28*28)`            | Flattens each image (28×28 pixels) into a **1D vector of 784 numbers**. `-1` means “figure out batch size automatically.”     | Like taking a **folded paper** (image) and laying it **flat** so the robot can read it in a line. |\n",
    "| `x = torch.relu(self.fc1(x))`      | Passes the input through the first layer (`fc1`), then applies **ReLU activation** (keeps positives, turns negatives into 0). | Like filtering signals — the robot ignores “negative” signals and only keeps useful ones.         |\n",
    "| `x = self.fc2(x)`                  | Feeds the result into the second layer (`fc2`) to get **10 output scores**.                                                   | Like the robot choosing which of the 10 digits it thinks the image is.                            |\n",
    "| `return x`                         | Returns the output predictions (called **logits**).                                                                           | Like the robot giving its answer: “I think this is a 7 (with 90% confidence).”                    |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9b6bc-95dc-422f-9985-e5ee90d63165",
   "metadata": {},
   "source": [
    "| Parameter in `nn.Linear(input_size, output_size)` | Meaning                                                                                  | Example in our case                                                       |\n",
    "| ------------------------------------------------- | ---------------------------------------------------------------------------------------- | ------------------------------------------------------------------------- |\n",
    "| `input_size`                                      | How many numbers come **into** the layer (like the number of doors into a room).         | `28*28 = 784` → Each MNIST image has 784 pixels (each pixel = one input). |\n",
    "| `output_size`                                     | How many numbers come **out** of the layer (like the number of windows out of the room). | `128` → The layer produces 128 features (compressed useful signals).      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "824f3769-a6da-48fe-8f00-fb4186ee2b30",
   "metadata": {},
   "source": [
    "| Parameter in `nn.Linear(128, 10)` | Meaning                                                      | Example in our case                               |\n",
    "| --------------------------------- | ------------------------------------------------------------ | ------------------------------------------------- |\n",
    "| `128`                             | Number of inputs to this layer (features coming from `fc1`). | 128 features summarizing the image.               |\n",
    "| `10`                              | Number of outputs from this layer.                           | 10 classes (digits 0–9).                          |\n",
    "| Output                            | What the layer produces.                                     | 10 raw values (logits), one score for each digit. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7831d26-2eb7-4047-bd81-f67e6b4610a3",
   "metadata": {},
   "source": [
    "**So the flow is:\n",
    "Pixels (784) → Features (128) → Digit Scores (10).**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eae836f-9e8f-4a97-adbc-2b4b7c55ca0b",
   "metadata": {},
   "source": [
    "### 🔎 What is a \"Digit Score\"?\n",
    "\n",
    "- When the network reaches the last layer (`fc2`), it produces **10 numbers**.  \n",
    "- Each number corresponds to one digit class:  \n",
    "  - First number → score for digit **0**  \n",
    "  - Second number → score for digit **1**  \n",
    "  - … and so on until digit **9**  \n",
    "\n",
    "These numbers are called **logits** in deep learning.  \n",
    "They are **not yet probabilities** — they can be positive, negative, or any value.  \n",
    "\n",
    "---\n",
    "\n",
    "### ⚡ Analogy\n",
    "\n",
    "Imagine you’re judging a handwriting contest with **10 possible winners (digits 0–9)**.  \n",
    "\n",
    "- For each digit, the judge (network) gives a **score** (like a raw rating).  \n",
    "\n",
    "**Example (logits):**\n",
    "\n",
    "- Digit 0 → -2.1  → The network thinks it doesn’t look like a 0.\n",
    "- Digit 1 → 0.5   → It looks a little like a 1.\n",
    "- Digit 7 → 3.7   → It looks strongly like a 7. \n",
    "- Digit 9 → 1.2   → It looks somewhat like a 9.\n",
    "\n",
    "Even though these aren’t probabilities, the **highest score wins** → here, digit **7**.  \n",
    "\n",
    "---\n",
    "\n",
    "### 🧠 Big Picture\n",
    "\n",
    "- **Digit scores = raw outputs (logits) from the final layer.**  \n",
    "- They represent how strongly the network thinks the input image belongs to each digit.  \n",
    "- The **highest score’s digit is the prediction**.  \n",
    "\n",
    "Later, we usually apply a **softmax function** to turn these raw scores into **probabilities** (e.g., “digit 7 = 85% likely”).  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9ed4f15-59f8-4fdf-952a-fa23e91bbf03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nExplanation of \\ndef forward(self,x):\\n    x = x.view(-1, 28*28)\\n    x = torch.relu(self.fc1(x))\\n    x = self.fc2(x)\\n    return x\\n    '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Explanation of \n",
    "def forward(self,x):\n",
    "    x = x.view(-1, 28*28)\n",
    "    x = torch.relu(self.fc1(x))\n",
    "    x = self.fc2(x)\n",
    "    return x\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68398f9-3fff-427f-9330-ca33739ee725",
   "metadata": {},
   "source": [
    "| **Line of Code**              | **What It Does**                                                                                                                  | **Simple Analogy**                                                                              |\n",
    "| ----------------------------- | --------------------------------------------------------------------------------------------------------------------------------- | ----------------------------------------------------------------------------------------------- |\n",
    "| `def forward(self, x):`       | Defines **how input data flows** through the network layers. This is called when you run `model(x)`.                              | Like writing down the **step-by-step instructions** for how your robot should process an image. |\n",
    "| `x = x.view(-1, 28*28)`       | Flattens the image (28×28 pixels) into a **single row of 784 numbers**. The `-1` automatically adjusts for batch size.            | Like unfolding a **folded newspaper** and laying it flat in one line so it’s easier to read.    |\n",
    "| `x = torch.relu(self.fc1(x))` | Sends the data through the **first layer** (`fc1`) and applies the **ReLU activation** (turns negatives into 0, keeps positives). | Like the robot filtering signals: “Ignore bad signals (negative) and keep useful ones.”         |\n",
    "| `x = self.fc2(x)`             | Passes the result into the **second layer** to get **10 output scores** (one for each digit 0–9).                                 | Like the robot saying: “Okay, I’ve processed the signals — here are my 10 possible guesses.”    |\n",
    "| `return x`                    | Returns the **final output** (raw predictions/logits).                                                                            | Like the robot handing you its final answer sheet.                                              |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9c6304-10ca-4bb2-9618-d13fb3bf1269",
   "metadata": {},
   "source": [
    "### 👉 In short:\n",
    "\n",
    "- Flatten the picture 📄\n",
    "\n",
    "- Process it with first layer + filter 🎛️\n",
    "\n",
    "- Send it through second layer 🔢\n",
    "\n",
    "- Give back predictions ✅"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1356e90-10fd-450f-806f-edfcaaa0affd",
   "metadata": {},
   "source": [
    "# 🏋️ Training vs 🧪 Testing in a Neural Network (Example with Digit \"5\")\n",
    "\n",
    "---\n",
    "\n",
    "## 🏋️ Training Phase (Learning Time)\n",
    "\n",
    "**Input:**\n",
    "- You feed an image of “5” into the network (28×28 pixels).\n",
    "\n",
    "**Forward Pass:**\n",
    "- The network flattens it → processes it through layers → outputs 10 raw scores (logits).\n",
    "- Example output: [ -0.8, 0.2, 0.1, -0.5, 0.3, 2.9, 0.7, -1.1, 0.0, -0.2 ]\n",
    "- The biggest score is at index **5** (2.9).  \n",
    "- Network predicts **“5”**.\n",
    "\n",
    "**Compare with True Label:**\n",
    "- We know the correct answer (label) is “5” because this is training data.\n",
    "- The model’s output is compared with the true label using a **loss function** (usually cross-entropy).\n",
    "\n",
    "**Error Calculation (Loss):**\n",
    "- If the network guessed wrong (say it thought “3”), the loss would be **high**.\n",
    "- If it guessed right (“5”), the loss is **low**.\n",
    "\n",
    "**Backpropagation + Optimizer:**\n",
    "- The optimizer (SGD/Adam, etc.) updates the network’s weights so it does **a little better next time**.\n",
    "- This repeats **thousands of times** until the network gets very good.\n",
    "\n",
    "---\n",
    "\n",
    "## 🧪 Testing Phase (Evaluation Time)\n",
    "\n",
    "**Input:**\n",
    "- Now you give a new image of “5” (from test set or even a random hand-drawn one).\n",
    "\n",
    "**Forward Pass:**\n",
    "- Same process: flatten → layers → 10 output scores.\n",
    "\n",
    "**Prediction:**\n",
    "- Pick the highest score as the network’s guess.\n",
    "- Example:[ -1.0, 0.5, 0.3, -0.4, 0.7, 3.8, 1.1, -0.6, 0.0, -0.2 ]\n",
    "- Highest is at index **5** (3.8).  \n",
    "- Prediction = **“5”**.\n",
    "\n",
    "**No Backpropagation:**\n",
    "- In testing, we **don’t adjust weights** anymore.\n",
    "- We only check: *Did the model’s guess match the true answer?*\n",
    "\n",
    "---\n",
    "\n",
    "## ⚖️ Analogy\n",
    "\n",
    "- **Training**: Like a student practicing math problems. If they get one wrong, the teacher explains the mistake, and the student **learns** (updates their brain).  \n",
    "- **Testing**: Like the final exam. The student just writes answers — no feedback, no learning. Only **grading** happens.\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ Summary\n",
    "\n",
    "- During **training**, the model learns from its mistakes by comparing predictions with correct labels.  \n",
    "- During **testing**, the model only predicts and we check accuracy — no learning happens.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bc4b540-4fed-43a0-9fdb-7f755da0f286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss, optimizer\n",
    "model = SimpleNN()\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de1c24b6-d566-460e-9142-e73333827e12",
   "metadata": {},
   "source": [
    "| **Line of Code**                                       | **What It Does**                                                                                                                                                                                 | **Simple Analogy**                                                                                                                                                           |\n",
    "| ------------------------------------------------------ | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ | ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |\n",
    "| `model = SimpleNN()`                                   | Creates an object of the `SimpleNN` class (the neural network we defined).                                                                                                                       | Like **building the robot** from your blueprint so it’s ready to work.                                                                                                       |\n",
    "| `loss_fn = nn.CrossEntropyLoss()`                      | Defines the **loss function**. Cross-entropy is used for classification tasks (like MNIST digits). It measures how far the predictions are from the correct answer.                              | Like a **teacher** checking how wrong your homework answers are.                                                                                                             |\n",
    "| `optimizer = optim.Adam(model.parameters(), lr=0.001)` | Sets up the **optimizer** (Adam). It will update the model’s parameters (weights) using the gradients during training. The `lr=0.001` is the learning rate — how big each update step should be. | Like giving the student a **study strategy**: how quickly to learn from mistakes. A small learning rate = small careful steps, a big one = big jumps (risk of overshooting). |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54d9af6f-856e-448a-9959-5499e54d11ec",
   "metadata": {},
   "source": [
    "# Optimizer & Learning Rate Explained (Simple Terms)\n",
    "\n",
    "### 1. What is Adam?\n",
    "- **Adam = Adaptive Moment Estimation** (an optimizer).\n",
    "- It’s the algorithm that **updates the weights** of your neural network after each batch.\n",
    "- Think of it as a **smart tutor** who:\n",
    "  - 📒 Remembers your past mistakes (**momentum**).\n",
    "  - 🔧 Adjusts how much to correct based on how hard the mistake was (**adaptive learning**).\n",
    "- Compared to older optimizers (like SGD), Adam usually **learns faster and smoother**.\n",
    "\n",
    "---\n",
    "\n",
    "### 2. What is Learning Rate (lr)?\n",
    "- **Learning rate = step size** for each weight update.\n",
    "- Analogy: Learning to ride a bike 🚲\n",
    "  - **High learning rate** → you make huge corrections → fast, but you wobble/fall.\n",
    "  - **Low learning rate** → tiny corrections → slow, but more stable.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Should it be higher or lower?\n",
    "- **High lr (e.g., 0.1)**:\n",
    "  - 🚀 Learns quickly.\n",
    "  - ❌ Might overshoot and never settle (too jumpy).\n",
    "- **Low lr (e.g., 0.0001)**:\n",
    "  - 🐢 Learns slowly.\n",
    "  - ✅ More precise and stable.\n",
    "\n",
    "---\n",
    "\n",
    "### 4. What’s a good value?\n",
    "- **Default = 0.001** → balanced (like in your code).\n",
    "- Tune if needed:\n",
    "  - If model **isn’t learning at all** → 🔼 increase lr.\n",
    "  - If model **bounces/oscillates** → 🔽 decrease lr.\n",
    "\n",
    "---\n",
    "\n",
    "📌 **Quick Analogy Recap**:\n",
    "- **Optimizer** = how you correct yourself while riding a bike.  \n",
    "- **Learning rate** = how big those corrections are.  \n",
    "- **Adam** = a smart rider who remembers past wobbles and improves correction each time.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c738a8e-abee-421d-bfc2-668e81dfc84a",
   "metadata": {},
   "source": [
    "# 📉 What is a Loss Function?\n",
    "\n",
    "A **loss function** tells the model how wrong its prediction was.  \n",
    "\n",
    "- The **smaller the loss** → the better the model is doing.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Why CrossEntropyLoss?\n",
    "\n",
    "- It’s the most common loss function for **classification problems** (like MNIST digit recognition).  \n",
    "- It combines **Softmax + Negative Log Likelihood (NLL)** in one step.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 How it Works\n",
    "\n",
    "### 1. Logits (raw model outputs)\n",
    "Example → `[-2.1, 0.5, 3.7, 1.2]`\n",
    "\n",
    "### 2. Softmax → Probabilities\n",
    "Converts logits into numbers between 0 and 1 (that sum to 1).  \n",
    "After softmax → `[0.02, 0.05, 0.85, 0.08]`\n",
    "\n",
    "### 3. Compare with True Label\n",
    "- Suppose the correct digit is **2** (index = 2).  \n",
    "- The probability for class 2 is **0.85**.  \n",
    "\n",
    "👉 If the probability was **low**, the loss would be **high** → punishing the model.  \n",
    "👉 If the probability was **high**, the loss would be **small** → rewarding the model.  \n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 Intuition\n",
    "\n",
    "Think of it like a **teacher grading multiple-choice answers**:  \n",
    "\n",
    "- The model writes its confidence for each option.  \n",
    "- The loss function checks if the model gave **high confidence to the correct answer**.  \n",
    "\n",
    "✅ If yes → **small penalty (good job)**  \n",
    "❌ If no → **large penalty (bad job)**  \n",
    "\n",
    "---\n",
    "\n",
    "## ✅ In Short\n",
    "`CrossEntropyLoss` measures **how far the predicted probability distribution is from the true answer**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af8774fc-dbaa-4fb2-841f-a3be10afb37b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss = 0.1303\n",
      "Epoch 2, Loss = 0.1749\n",
      "Epoch 3, Loss = 0.0333\n",
      "Epoch 4, Loss = 0.1395\n",
      "Epoch 5, Loss = 0.1421\n",
      "Epoch 6, Loss = 0.0163\n",
      "Epoch 7, Loss = 0.0027\n",
      "Epoch 8, Loss = 0.1560\n",
      "Epoch 9, Loss = 0.0148\n",
      "Epoch 10, Loss = 0.0053\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "for epoch in range(10):   \n",
    "    for images, labels in train_loader:\n",
    "        outputs = model(images)         \n",
    "        loss = loss_fn(outputs, labels)   \n",
    "\n",
    "        optimizer.zero_grad()             \n",
    "        loss.backward()                   \n",
    "        optimizer.step()                  \n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b3648b5-b9f9-4a26-b491-e3d0e4108dd7",
   "metadata": {},
   "source": [
    "| Line of Code                                          | What It Does                                                                           | Analogy                                                                     |\n",
    "| ----------------------------------------------------- | -------------------------------------------------------------------------------------- | --------------------------------------------------------------------------- |\n",
    "| `for epoch in range(10):`                             | Runs the training process for 10 full passes through the dataset (10 epochs).          | Like practicing the entire question paper 10 times.                         |\n",
    "| `for images, labels in train_loader:`                 | Loops through batches of training data (images + their correct labels).                | Getting practice questions in small sets instead of the whole exam at once. |\n",
    "| `outputs = model(images)`                             | Passes the batch of images through the neural network to get predictions (logits).     | The student attempts answers based on their current knowledge.              |\n",
    "| `loss = loss_fn(outputs, labels)`                     | Calculates how far the predictions are from the correct answers using a loss function. | Teacher checks answers and gives a penalty score (higher = more mistakes).  |\n",
    "| `optimizer.zero_grad()`                               | Resets (clears) old gradients before calculating new ones.                             | Erasing yesterday’s corrections before starting today’s work.               |\n",
    "| `loss.backward()`                                     | Backpropagation: calculates gradients (how much each parameter should change).         | Teacher gives feedback on where and how much the student went wrong.        |\n",
    "| `optimizer.step()`                                    | Updates the model’s parameters using the optimizer (like Adam).                        | Student improves their understanding based on feedback.                     |\n",
    "| `print(f\"Epoch {epoch+1}, Loss = {loss.item():.4f}\")` | Displays the loss value after each epoch to monitor progress.                          | Seeing the student’s score improve after every round of practice.           |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b452558b-7593-4136-a3e8-d8729c833f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy = 97.58%\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "correct, total = 0, 0\n",
    "with torch.no_grad():  # no gradients during testing\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"Test Accuracy = {100 * correct / total:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d89034-4edc-450e-aeaa-79133d46f13f",
   "metadata": {},
   "source": [
    "| Line of Code                                             | What It Does                                                       | Analogy                                                                                  |\n",
    "| -------------------------------------------------------- | ------------------------------------------------------------------ | ---------------------------------------------------------------------------------------- |\n",
    "| `correct, total = 0, 0`                                  | Initializes counters for correct predictions and total samples.    | Starting with a clean scorecard before grading an exam.                                  |\n",
    "| `with torch.no_grad():`                                  | Turns off gradient calculation (saves memory & speeds up testing). | During the real exam, no teacher feedback/corrections are given — just checking answers. |\n",
    "| `for images, labels in test_loader:`                     | Loops over batches of test data.                                   | Distributes exam papers to students in groups.                                           |\n",
    "| `outputs = model(images)`                                | Gets model predictions for the test batch.                         | Student writes answers based on their learning.                                          |\n",
    "| `_, predicted = torch.max(outputs, 1)`                   | Selects the class with the highest score (the model’s choice).     | Student picks the answer they are most confident about.                                  |\n",
    "| `correct += (predicted == labels).sum().item()`          | Counts how many answers are correct in this batch.                 | Teacher marks the number of correct answers.                                             |\n",
    "| `total += labels.size(0)`                                | Keeps track of total number of questions checked.                  | Counting total questions in the exam.                                                    |\n",
    "| `print(f\"Test Accuracy = {100 * correct / total:.2f}%\")` | Calculates and prints accuracy = (correct ÷ total × 100).          | Final exam result sheet: percentage score.                                               |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ca6e3d-39ee-4b12-935a-0914fb373f7b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
